\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage[right=0.8in, top=1in, bottom=1.2in, left=0.8in]{geometry}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Blue,linkcolor=BrickRed]{hyperref}
\usepackage[dvipsnames,usenames]{xcolor}

%\spacing{1.06}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{\vspace{0.25cm}
      \hbox to 5.78in { {University of Wrocław:\hspace{0.12cm}Algorithms for
          Big Data (Fall'19)} \hfill #2 }
      \vspace{0.48cm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{0.42cm}
      \hbox to 5.78in { {#3 \hfill #4} }\vspace{0.25cm}
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe:\hspace{0.08cm}#4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\newcommand{\bigo}{\mathcal{O}}

\begin{document}

\lecture{5: Dimensionality reduction}{ 04/11/2019}{Lecturer: \emph{Przemysław Uznański }}{}

\section{Dimensionality reduction recap}
Two versions of the JL lemma.
\begin{theorem}[Distributional JL]
For any integer $n$, and $0 < \varepsilon, \delta < \frac12$, and $m = \bigo(\log(1/\delta) / \eps^2)$ there is distribution $\mathcal{D}$ over matrices $\mathbb{R}^{m \times n}$ such that for every $x \in \mathbb{R}^n$ such that $\|x\|_2 = 1$:
$$\Pr_{\Pi \sim \mathcal{D}} ( | \Pi \cdot x |_2^2 - 1  > \eps) < \delta $$
\end{theorem}


note: we fix the distribution, and it works for all the vectors

implies

\begin{theorem}[Metric JL]
For $X$, a set of $n$ points in dimension $n$, there exists linear $f : X \to \mathbb{R}^m$ for $m = \bigo(\log(n) / \eps^2)$ that preserves distances approximately, that is
$$\forall_{i,j} (1-\eps) |x_i - x_j|_2 \le |f(x_i) - f(x_j)|_2 \le (1+\eps) |x_i - x_j|_2.$$
\end{theorem}

follows from DJL by setting $\delta < 1/n^2$ and taking union bound


we already know that:
$N(0,1) \cdot 1/\sqrt{m}$ iid coefficients work
scaled rademacher $+1/-1   \cdot 1/sqrt(m)$ iid coefficients work


JL is a dimensionality reduction in L2 norm. Problem: applying a single projection takes $\bigo(nm)$ time.

\section{Fast JL \cite{DBLP:journals/siamcomp/AilonC09}}

Main idea is to structurize the matrix, so the time to apply matrix ~ $\bigo(n+m log m)$

$\Pi = \frac{1}{\sqrt{m}}  \cdot S \cdot H \cdot D$

where
\begin{itemize}
\item $S$ is $n \times m$ sampling matrix (each row has single 1 in random position, rows are independent)
\item $H$ is a $m \times m$ Fourier matrix or Hadamard matrix (we need $H H = I$ and $\max_{i,j} |H_{i,j}| \le 1/\sqrt{m} $
\item $D$ is a $m \times m$ $\textrm{diag}(\sigma)$ where $\sigma$ is a vector of independent Rademachers
\end{itemize}

$D$ applies in time $\bigo(m)$, $H$ applies in time $\bigo(m log m)$, $S$ applies in time $\bigo(n)$



\begin{theorem}
For $m = \bigo(\log(1/\delta) \cdot \log(n/\delta) \cdot \eps^{-2})$ and $\|x\|_2 = 1$ we have 
$$\Pr_{\Pi} ( | \Pi \cdot x |_2^2 - 1  > \eps) < \delta $$
\end{theorem}

We will need:
\begin{theorem}[Khintchine inequality]
For any $p \ge 1$, $x \in \mathbb{R}^n$ and $(\sigma_i)$ independent Rademacher,
$$ \left( \E \left|\sum_i x_i \sigma_i\right|^p \right)^{1/p} \le \bigo(\sqrt{p}) \| x \|_2 $$
\end{theorem}
\begin{theorem}[Chernoff bound]
$X_1,\ldots,X_n$ are independent random variables and $X_i \in [0,\tau]$. Let $\mu = \E \sum_i X_i $. Then
$$\Pr[ | \sum_i X_i - \mu| > \eps \mu ] < 2 \exp(- \frac{\eps^2 \mu}{2 \tau} )$$
\end{theorem}

\begin{proof}[Proof of main result]
Denote $y = \frac{1}{\sqrt{n}}HDx$ and $z = \sqrt{\frac{n}{m}} \cdot S \cdot y$.


First ur goal is to show  bound on $\| y \|_{\infty}$.

$$y_i = (\frac{1}{\sqrt{n}}HDx)_i = \sum_{j=1}^n \sigma_j \frac{1}{\sqrt{n}} \gamma_{i,j} x_j = \sigma \odot w^{(i)}$$
where $w^{(i)}$ is a vector $w^{(i)}_j = \frac{1}{\sqrt{n}} \gamma_{i,j}x_j$.

First, $\|y\|_2 = \|x\|_2 = 1$ since (normalized) Hadamard transform preserves $L_2$ norms, but one can also prove that $\E |y_i|^2 = \E | \sigma \odot w^{(i)} |^2  = \|w^{(i)}\|_2^2 = \sum_j (\frac{1}{\sqrt{n}} \gamma_{i,j} x_j)^2 = \frac{1}{n} \|x\|_2 = \frac{1}{n}$ where we only used that $H$ has $-1/+1$ coefficients and that $\sigma$ is Rademacher.


By Khintchine, and using length of $x$ and fact that $\gamma_{i,j} \in \{-1,+1\}$, for some absolute constant $C$:\footnote{We will denote with $C,C',C'',\ldots$ an absolute constants.}
$$\E |y_i|^p \le \left(C \sqrt{p} \| w^{(i)} \|_2\right)^{p} = \left(\sqrt{\frac{\bigo(p)}{n}}\right)^p$$
by Markov's inequality, for any $\beta$
$$\Pr\left[ |y_i| \ge \sqrt{\frac{\bigo(p)}{n}} \cdot \left( \frac{2n}{\delta} \right)^{1/p}\right] = \Pr\left[ |y_i|^p \ge  \left(C \cdot \sqrt{\frac{p}{n}}\right)^p \cdot \frac{2n}{\delta} \right] \le \frac{\delta}{2n}$$

Optimizing, the term
$$\varphi(p) =  \sqrt{\frac{\bigo(p)}{n}} \cdot \left( \frac{2n}{\delta} \right)^{1/p} \sim \exp\left(\bigo(1)+\frac{1}{2} \ln p + \frac{1}{p} \ln \left( \frac{2n}{\delta} \right) \right)$$
minimizes when $p \approx  2 \ln \left( \frac{2n}{\delta} \right) $, so then $ \varphi_{\min}(p)= \sqrt{\frac{\bigo(p)}{n}}\cdot \sqrt{e}$ and so
$$\Pr\left[ |y_i| \ge  \bigo\left( \sqrt{\frac{\ln (2n/\delta)}{n}} \right)\right] \le \frac{\delta}{2n},$$
and taking the union bound
$$\Pr\left[ \|y\|_\infty \ge  \bigo\left(\sqrt{\frac{\ln (2n/\delta)}{n}}\right) \right] \le \frac{\delta}{2}.$$
So in the following we condition on the event that $\| y \|_\infty = \bigo\left(\sqrt{\frac{\ln (2n/\delta)}{n}}\right)$.

Now consider $X_i = (z_i)^2$ as a random variable. We have the following:
$$\E X_i = \E (z_i)^2 = \frac{n}{m} \cdot \frac{\|y\|_2^2}{n} = \frac{1}{m}.$$
And from bound on $\|y\|_\infty$ there is $X_i \le \frac{n}{m} \cdot \bigo(\frac{\ln(2n/\delta)}{n} )= \bigo(\frac{\ln(2n/\delta)}{m}) = \tau$.


We now apply Chernoff bound, with $\mu = 1$ (keep in mind that $\sum_i X_i = \|z\|_2^2$)

$$\Pr[ | \sum_i X_i - 1| > \eps  ] < 2 \exp(- \frac{\eps^2 }{2 \tau} ) = 2 \exp\left(- \frac{\eps^2 m}{\bigo(\ln(2n/\delta)) }\right)$$


Now, usually when applying Chernoff bound, $\tau=1$ and then it is enough to set $m = \bigo(\varepsilon^{-2} \log (1/\delta))$ to have the probability in the bound be $\delta$. Unfortunately in our case, $\tau \gg 1$ so we have to offset for the log term in the denominator. So we need to set 
$m = \bigo(\varepsilon^{-2} \log (1/\delta) \log(n/\delta)$ so we can bound the probability in the Chernoff by $\delta/2$.

Taking union bound over both $\delta/2$ failure probability finishes the proof.
\end{proof}

Using this approach, this dependency is roughly optimal (that is, we are losing one log vs. optimal JL). However, one trick to reduce $\eps$ to optimal at the cost of slower runtime is to apply $m' \times m$ ''naive'' JL at the end of the chain of matrices, for $m' = \bigo(\log (1/\delta) \eps^{-2})$. This adds $\bigo(m' \cdot m) = \bigo(\eps^{-4} \log^2 (1/\delta) \log (n/\delta))$ to the running time though.

\section{Sparse JL \cite{DBLP:conf/stoc/DasguptaKS10}}

Motivation: if $x$ is sparse (that is, $\|x\|_0$ is small), we expect time proportional to $\|x\|_0$. 

We consider distributions $\mathcal{D}$ of matrices such that:
\begin{itemize}
\item Each column has only $s$ non-zero elements, for some $s$. (Either deterministically or in expectation.)
\item They still provide good dimensionality reduction.
\end{itemize}

The time to compute $\Pi x$ is then $s \cdot \|x\|_0$, since each column determines ''where'' each $x_i$ contributes and can be processed in $\bigo(s)$ time.

\subsection{Dasgupta etal. construction}
$s = \bigo(\eps^{-1} \log (1/\delta) \log^2 (m/\delta))$,\\ $h : [sn] \to [m]$ be a random hash function , and let
$H \in \{-1,0,1\}^{m \times sn}$ be such that $H_{ij} = \delta_{i,h(j)} r_j$. (all $r$ are indep. Rademacher).
$P \in \{0,1\}^{sn \times n}$ be such that
$$P_{i,j} = \begin{cases}1 \text{ for } (j-1)s + 1 \le i \le js\\0 \end{cases}$$

Intuition:
$P$ creates $s$ copies of each element of input, $H$ hashes each element (after duplication) into $[m]$ together with $+1/-1$ coef. This can be evaluated implicitly without expanding the matrices.


\begin{theorem}
$\Pi = \frac{1}{\sqrt{s}}HP$ has JL guarantees. $\Pi x$ can be evaluated in time $\bigo(s \|x\|_0)$.
\end{theorem}
We skip the proof.

\subsection{Kane, Nelson construction}
$s = \bigo(\eps m) = \bigo(\varepsilon^{-1} \log(1/\delta))$

Construction 1: matrix $m \times n$, where in each column we place $s$ random $-1/+1$ (sample without replacement), normalized with $\frac{1}{\sqrt{s}}$ coef.\\
Construction 2: group each column into $s$ blocks, each of size $m/s$. Pick in each block one $-1/+1$, normalize with $\frac{1}{\sqrt{s}}$ coef.\\

Construction 2 is effectively the same as \textsf{CountSketch}. The proof that it works shows that analysis using more than just 2-independence can show a very good concentration (CountSketch uses median, here we can use average).


\section{Missing proofs}
\begin{proof}[Proof of Khintchine Inequality]
For any variable $X$, $\E[|X|^p]^{1/p}$ is increasing with $p$ (see: generalized average inequality), so we can round-up $p$ to even integer. Consider $g_i \sim \mathcal{N}(0,1)$.
Expand $\E[ (\sum_i \sigma_i x_i)^p]$ into sum of monomials. Any monomial with odd-exponents vanishes. Similarly in $\E[ (\sum_i g_i x_i)^p ]$. For any even-exponents $\alpha_1,\ldots$, $\E \prod_i \sigma_i^{\alpha_i} = 1$ while $\E \prod_i g_i^{\alpha_i} \ge 1$, so the gaussian case dominates the rademacher case.\footnote{Useful fact is that for $g \sim \mathcal{N}(0,1)$ and even $p$, $\E[g^p] = (p-1)!!$.}

But $\sum_i g_i x_i$ is itself normal variable $\mathcal{N}(0,\|x\|_2^2)$, so
$$\E[ (\sum_i \sigma_i x_i)^p ] \le \E[ (\sum_i g_i x_i)^p ] =  (p-1)!! \|x\|_2^p$$
Asymptotically, $((p-1)!!)^{1/p} \approx (2^{p/2} \cdot (p/2)!)^{1/p} \approx \sqrt{2} \cdot \left(\frac{(p/2)^{p/2}}{e^{p/2}} \right)^{1/p} = \sqrt{\frac{p}{e}}$, so the hidden constant in Khintchine inequality is $\bigo(\sqrt{p})$.

\end{proof}

\bibliographystyle{alpha}
\bibliography{bib}
\end{document}


